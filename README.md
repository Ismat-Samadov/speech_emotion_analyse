# Speech Emotion Recognition (SER)

> A deep learning project for recognizing emotions from speech audio using multiple datasets and comprehensive feature extraction.

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.16-orange.svg)](https://www.tensorflow.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Dataset](#dataset)
- [Exploratory Data Analysis](#exploratory-data-analysis)
- [Model Architecture](#model-architecture)
- [Training Process](#training-process)
- [Performance Results](#performance-results)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Contributing](#contributing)

---

## ğŸ¯ Overview

This project implements a deep neural network to classify emotions from speech audio files. The model is trained on **12,162 audio samples** from 4 popular datasets and achieves **64% accuracy** across 8 emotion classes.

### Key Features

- âœ… Multi-dataset training (RAVDESS, CREMA-D, TESS, SAVEE)
- âœ… Comprehensive audio feature extraction (MFCC, Chroma, Mel Spectrogram, etc.)
- âœ… Deep neural network with 277K parameters
- âœ… Professional visualizations and analysis
- âœ… Organized outputs (charts, metrics, model artifacts)
- âœ… Production-ready model artifacts

### Emotions Recognized

The model can classify speech into 8 distinct emotions:
- **Angry** ğŸ˜¡
- **Calm** ğŸ˜Œ
- **Disgust** ğŸ¤¢
- **Fear** ğŸ˜¨
- **Happy** ğŸ˜Š
- **Neutral** ğŸ˜
- **Sad** ğŸ˜¢
- **Surprise** ğŸ˜²

---

## ğŸ“Š Dataset

### Dataset Sources

The project combines 4 publicly available speech emotion datasets:

| Dataset | Samples | Emotions | Description |
|---------|---------|----------|-------------|
| **RAVDESS** | 1,440 | 8 emotions | Ryerson Audio-Visual Database |
| **CREMA-D** | 7,442 | 6 emotions | Crowd-sourced Emotional Multimodal Actors |
| **TESS** | 2,800 | 7 emotions | Toronto Emotional Speech Set |
| **SAVEE** | 480 | 7 emotions | Surrey Audio-Visual Expressed Emotion |
| **Total** | **11,402*** | 8 unique | *After preprocessing |

### Dataset Distribution

![Dataset Distribution](charts/02_dataset_distribution_pie.png)

**Statistics:**
- Total audio files found: **12,162**
- Successfully processed: **11,402** (93.8%)
- Skipped/corrupted: **760** (6.2%)

---

## ğŸ” Exploratory Data Analysis

### Emotion Distribution

The dataset shows a relatively balanced distribution across emotions, with some natural variations:

![Emotion Distribution](charts/01_emotion_distribution.png)

**Emotion Counts:**
- Disgust: ~1,900 samples
- Fear: ~1,800 samples
- Happy: ~1,800 samples
- Angry: ~1,800 samples
- Neutral: ~1,500 samples
- Sad: ~1,800 samples
- Calm: ~200 samples
- Surprise: ~200 samples

### Emotion Distribution by Dataset

Different datasets contribute different emotions:

![Emotion by Dataset - Stacked](charts/03_emotion_by_dataset_stacked.png)

![Emotion-Dataset Heatmap](charts/04_emotion_dataset_heatmap.png)

**Key Insights:**
- CREMA-D provides the most diverse emotion samples
- RAVDESS includes unique "calm" emotion
- TESS focuses on clear, distinct emotions
- SAVEE adds balanced representation across all emotions

---

## ğŸ—ï¸ Model Architecture

### Network Structure

The model is a **Sequential Deep Neural Network** with the following architecture:

```
Input Layer (195 features)
    â†“
Dense(512) + ReLU + Dropout(0.3) + BatchNorm
    â†“
Dense(256) + ReLU + Dropout(0.3) + BatchNorm
    â†“
Dense(128) + ReLU + Dropout(0.3) + BatchNorm
    â†“
Dense(64) + ReLU + Dropout(0.2) + BatchNorm
    â†“
Dense(8) + Softmax
    â†“
Output (8 emotion classes)
```

### Model Summary

```
Total Parameters: 277,192
Trainable Parameters: 275,272
Non-trainable Parameters: 1,920
Model Size: ~1.06 MB
```

### Feature Extraction

**195 audio features** extracted from each 3-second audio clip:

| Feature Type | Count | Description |
|--------------|-------|-------------|
| **MFCC** | 40 | Mel-frequency cepstral coefficients |
| **Chroma** | 12 | Pitch class profiles |
| **Mel Spectrogram** | 128 | Frequency representation |
| **Spectral Contrast** | 7 | Peak-valley differences |
| **Tonnetz** | 6 | Tonal centroid features |
| **Zero Crossing Rate** | 1 | Signal polarity changes |
| **Spectral Rolloff** | 1 | Frequency energy distribution |

---

## ğŸ“ˆ Training Process

### Training Configuration

- **Optimizer**: Adam (learning_rate=0.001)
- **Loss Function**: Categorical Crossentropy
- **Batch Size**: 32
- **Max Epochs**: 100
- **Early Stopping**: Patience=15 (monitor validation loss)
- **Learning Rate Reduction**: Factor=0.5, Patience=5

### Data Split

- **Training Set**: 7,981 samples (70%)
- **Validation Set**: 1,710 samples (15%)
- **Test Set**: 1,711 samples (15%)

### Training History

The model was trained with automatic checkpointing and early stopping:

![Training History](charts/06_training_history.png)

**Training Results:**
- Training converged after ~30-40 epochs
- Best validation accuracy: ~65%
- Final test accuracy: **63.94%**
- Test loss: **0.9965**

---

## ğŸ¯ Performance Results

### Overall Metrics

| Metric | Value |
|--------|-------|
| **Test Accuracy** | **63.94%** |
| **Test Loss** | 0.9965 |
| **Macro Average Precision** | 65% |
| **Macro Average Recall** | 63% |
| **Macro Average F1-Score** | 63% |

### Confusion Matrix

![Confusion Matrix](charts/07_confusion_matrix.png)

![Normalized Confusion Matrix](charts/09_confusion_matrix_normalized.png)

### Per-Class Performance

![Per-Class Accuracy](charts/08_per_class_accuracy.png)

**Detailed Metrics:**

| Emotion | Precision | Recall | F1-Score | Accuracy | Support |
|---------|-----------|--------|----------|----------|---------|
| **Angry** | 79% | 72% | 75% | 72.16% | 291 |
| **Calm** | 59% | 82% | 68% | 81.82% | 33 |
| **Disgust** | 52% | 65% | 58% | 64.93% | 288 |
| **Fear** | 66% | 61% | 64% | 60.58% | 274 |
| **Happy** | 65% | 56% | 60% | 55.71% | 289 |
| **Neutral** | 62% | 69% | 65% | 68.83% | 231 |
| **Sad** | 64% | 61% | 62% | 60.57% | 279 |
| **Surprise** | 75% | 35% | 47% | 34.62% | 26 |

![Precision, Recall, F1-Score](charts/10_precision_recall_f1.png)

### Test Set Distribution

![Test Set Distribution](charts/11_test_set_distribution.png)

### Key Findings

âœ… **Strong Performance:**
- Angry emotion: 72% accuracy (best performer)
- Calm emotion: 82% accuracy (small sample size)
- Neutral emotion: 69% accuracy

âš ï¸ **Challenges:**
- Surprise emotion: 35% accuracy (limited training data)
- Disgust vs. Angry confusion (similar acoustic features)
- Happy vs. Calm overlap

---

## ğŸµ Sample Predictions

### Audio Analysis Example

The model provides detailed analysis for each audio file:

![Sample Audio Analysis](charts/12_audio_sample_DC_n13.png)

**Components:**
1. **Waveform** - Time-domain representation
2. **Spectrogram** - Frequency content over time
3. **MFCC** - Mel-frequency cepstral coefficients
4. **Prediction Probabilities** - Confidence for each emotion

**Example Prediction:**
```
File: DC_n13.wav
Actual Emotion: disgust
Predicted Emotion: disgust
Confidence: 99.74%

Probability Distribution:
  Disgust: 99.74%
  Fear: 0.08%
  Happy: 0.06%
  Sad: 0.04%
  Neutral: 0.04%
  Angry: 0.04%
  Surprise: 0.00%
  Calm: 0.00%
```

---

## ğŸš€ Usage

### Quick Start

```python
import pickle
from tensorflow import keras
import librosa
import numpy as np

# Load trained model and preprocessors
model = keras.models.load_model('artifacts/speech_emotion_recognition_model.keras')
scaler = pickle.load(open('artifacts/scaler.pkl', 'rb'))
label_encoder = pickle.load(open('artifacts/label_encoder.pkl', 'rb'))

# Load and predict emotion from audio file
audio, sr = librosa.load('path/to/audio.wav', duration=3, sr=22050)

# Extract features (use the extract_features function from notebook)
features = extract_features('path/to/audio.wav')
features_scaled = scaler.transform(features.reshape(1, -1))

# Predict
prediction = model.predict(features_scaled)
emotion = label_encoder.inverse_transform([np.argmax(prediction)])[0]
confidence = np.max(prediction) * 100

print(f"Predicted Emotion: {emotion}")
print(f"Confidence: {confidence:.2f}%")
```

### Batch Prediction

```python
import pandas as pd

# Predict emotions for multiple files
audio_files = ['file1.wav', 'file2.wav', 'file3.wav']
results = []

for file in audio_files:
    features = extract_features(file)
    features_scaled = scaler.transform(features.reshape(1, -1))
    prediction = model.predict(features_scaled, verbose=0)
    emotion = label_encoder.inverse_transform([np.argmax(prediction)])[0]

    results.append({
        'file': file,
        'emotion': emotion,
        'confidence': np.max(prediction) * 100
    })

df = pd.DataFrame(results)
print(df)
```

---

## ğŸ“ Project Structure

```
speech_emotion_recognition/
â”œâ”€â”€ ğŸ“Š charts/                          # Visualization outputs (12+ charts)
â”‚   â”œâ”€â”€ 01_emotion_distribution.png
â”‚   â”œâ”€â”€ 02_dataset_distribution_pie.png
â”‚   â”œâ”€â”€ 03_emotion_by_dataset_stacked.png
â”‚   â”œâ”€â”€ 04_emotion_dataset_heatmap.png
â”‚   â”œâ”€â”€ 05_emotion_by_dataset_grouped.png
â”‚   â”œâ”€â”€ 06_training_history.png
â”‚   â”œâ”€â”€ 07_confusion_matrix.png
â”‚   â”œâ”€â”€ 08_per_class_accuracy.png
â”‚   â”œâ”€â”€ 09_confusion_matrix_normalized.png
â”‚   â”œâ”€â”€ 10_precision_recall_f1.png
â”‚   â”œâ”€â”€ 11_test_set_distribution.png
â”‚   â”œâ”€â”€ 12_audio_sample_*.png
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ ğŸ“„ outputs/                         # Data files and metrics
â”‚   â”œâ”€â”€ emotion_distribution.csv
â”‚   â”œâ”€â”€ dataset_distribution.csv
â”‚   â”œâ”€â”€ classification_report.csv
â”‚   â”œâ”€â”€ confusion_matrix.csv
â”‚   â”œâ”€â”€ per_class_accuracy.csv
â”‚   â”œâ”€â”€ training_history.csv
â”‚   â”œâ”€â”€ test_results.json
â”‚   â”œâ”€â”€ file_summary.txt
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ ğŸ¯ artifacts/                       # Trained models
â”‚   â”œâ”€â”€ speech_emotion_recognition_model.keras  # Final model
â”‚   â”œâ”€â”€ best_ser_model.keras                    # Best checkpoint
â”‚   â”œâ”€â”€ scaler.pkl                              # Feature scaler
â”‚   â”œâ”€â”€ label_encoder.pkl                       # Label encoder
â”‚   â”œâ”€â”€ model_metadata.json                     # Model specs
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ ğŸ“š dataset/                         # Audio datasets (not in git)
â”‚   â”œâ”€â”€ Crema/                          # 7,442 files
â”‚   â”œâ”€â”€ Ravdess/                        # 1,440 files
â”‚   â”œâ”€â”€ Savee/                          # 480 files
â”‚   â””â”€â”€ Tess/                           # 2,800 files
â”‚
â”œâ”€â”€ ğŸ““ speech_emotion_recognition.ipynb # Main Jupyter notebook
â”œâ”€â”€ ğŸ“‹ requirements.txt                 # Python dependencies
â”œâ”€â”€ ğŸ“– README.md                        # This file
â”œâ”€â”€ ğŸ“ ENHANCEMENTS.md                  # Enhancement documentation
â””â”€â”€ ğŸ”§ .gitignore                       # Git ignore rules
```

---

## ğŸ› ï¸ Installation

### Prerequisites

- Python 3.9, 3.10, 3.11, or 3.12 (Python 3.11 recommended)
- 4GB+ RAM
- ~2GB disk space (for dataset and models)

### Setup Instructions

1. **Clone the repository:**
   ```bash
   git clone https://github.com/Ismat-Samadov/speech_emotion_recognition.git
   cd speech_emotion_recognition
   ```

2. **Create virtual environment with Python 3.11:**
   ```bash
   python3.11 -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Download the dataset** (automatic via kagglehub):
   ```python
   import kagglehub
   path = kagglehub.dataset_download("dmitrybabko/speech-emotion-recognition-en")
   ```

5. **Run the notebook:**
   ```bash
   jupyter notebook speech_emotion_recognition.ipynb
   ```

### System Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| CPU | 2 cores | 4+ cores |
| RAM | 4 GB | 8+ GB |
| Storage | 2 GB | 5+ GB |
| Python | 3.9+ | 3.11 |

---

## ğŸ“Š Performance Benchmarks

### Training Time

- **Feature Extraction**: ~15-20 minutes (12,162 files)
- **Model Training**: ~5-10 minutes (100 epochs with early stopping)
- **Total Pipeline**: ~25-30 minutes

### Inference Speed

- Single prediction: ~50-100ms
- Batch (32 samples): ~500ms-1s
- Feature extraction: ~30-50ms per file

---

## ğŸ”¬ Technical Details

### Audio Processing

- **Sample Rate**: 22,050 Hz
- **Duration**: 3 seconds (zero-padded/truncated)
- **Channels**: Mono
- **Format**: WAV

### Feature Engineering

All features are normalized using StandardScaler to ensure consistent model input:

```python
# Feature extraction parameters
duration = 3  # seconds
sr = 22050    # sample rate
n_mfcc = 40   # MFCC coefficients
```

### Model Training

- **Regularization**: Dropout (0.2-0.3) + Batch Normalization
- **Activation**: ReLU (hidden layers), Softmax (output)
- **Weight Initialization**: Glorot Uniform
- **Callbacks**: EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

---

## ğŸ“ˆ Future Improvements

- [ ] Implement attention mechanisms for better feature learning
- [ ] Add data augmentation (noise, pitch shifting, time stretching)
- [ ] Experiment with CNN/LSTM architectures
- [ ] Increase training data for underrepresented emotions
- [ ] Deploy as REST API or web application
- [ ] Add real-time emotion recognition from microphone
- [ ] Multi-language support
- [ ] Gender and age classification

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

### Datasets

- **RAVDESS**: Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)
- **CREMA-D**: Cao H, Cooper DG, Keutmann MK, Gur RC, Nenkova A, Verma R (2014)
- **TESS**: Toronto Emotional Speech Set - University of Toronto
- **SAVEE**: Surrey Audio-Visual Expressed Emotion Database

### Tools & Libraries

- TensorFlow / Keras
- Librosa
- Scikit-learn
- Matplotlib / Seaborn
- NumPy / Pandas

---

## ğŸ“§ Contact

**Ismat Samadov**
- GitHub: [@Ismat-Samadov](https://github.com/Ismat-Samadov)
- Project Link: [https://github.com/Ismat-Samadov/speech_emotion_recognition](https://github.com/Ismat-Samadov/speech_emotion_recognition)

---

## â­ Star History

If you find this project useful, please consider giving it a star!

---

<div align="center">

**Built with â¤ï¸ using Claude Code**

[â¬† Back to Top](#speech-emotion-recognition-ser)

</div>
